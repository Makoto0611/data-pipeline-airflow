# ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£… - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ç‰ˆ
# ç›®çš„: PostgreSQLã®salesãƒ‡ãƒ¼ã‚¿ã‚’æ—¥ä»˜åˆ¥ã«é›†è¨ˆã—ã¦BigQueryã¸é€ã‚‹
# Phase 3: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„æ©Ÿèƒ½ã‚’è¿½åŠ 
# å­¦ç¿’æ—¥: 2026-01-26

"""
========================================
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ãƒã‚¤ãƒ³ãƒˆä¸€è¦§
========================================

ã€æ”¹å–„â‘ ã€‘ã‚¯ã‚¨ãƒªæœ€é©åŒ–
    å ´æ‰€: extract_from_postgresé–¢æ•°ï¼ˆ67è¡Œç›®ä»˜è¿‘ï¼‰
    å†…å®¹: SELECT * â†’ SELECT id, sale_date, amount
    åŠ¹æœ: ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å–å¾—ã—ãªã„ã“ã¨ã§ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è»¢é€é‡ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›

ã€æ”¹å–„â‘¡ã€‘å‡¦ç†æ™‚é–“è¨ˆæ¸¬
    å ´æ‰€: ã™ã¹ã¦ã®é–¢æ•°ï¼ˆextract/transform/check/loadï¼‰
    å†…å®¹: time.time()ã§å„å‡¦ç†ã®é–‹å§‹ãƒ»çµ‚äº†æ™‚åˆ»ã‚’è¨˜éŒ²
    åŠ¹æœ: ã©ã®å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–

ã€æ”¹å–„â‘¢ã€‘ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¨ˆæ¸¬
    å ´æ‰€: ã™ã¹ã¦ã®é–¢æ•°ï¼ˆextract/transform/check/loadï¼‰
    å†…å®¹: psutil.Process()ã§å„å‡¦ç†ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
    åŠ¹æœ: ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãƒªã‚¹ã‚¯ã‚’äº‹å‰ã«æ¤œçŸ¥

ã€æ”¹å–„â‘£ã€‘æ”¹å–„åŠ¹æœã‚µãƒãƒªãƒ¼
    å ´æ‰€: load_to_postgresé–¢æ•°ã®æœ€å¾Œï¼ˆæœ€çµ‚è¡Œä»˜è¿‘ï¼‰
    å†…å®¹: å…¨å‡¦ç†ã®åˆè¨ˆæ™‚é–“ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‡ºåŠ›
    åŠ¹æœ: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ä¸€ç›®ã§æŠŠæ¡
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
import psycopg2
import pandas as pd
from sqlalchemy import create_engine
import logging
import time  # ã€æ”¹å–„â‘¡ã€‘å‡¦ç†æ™‚é–“è¨ˆæ¸¬ç”¨
import psutil  # ã€æ”¹å–„â‘¢ã€‘ãƒ¡ãƒ¢ãƒªè¨ˆæ¸¬ç”¨
import os  # ã€æ”¹å–„â‘¢ã€‘ãƒ—ãƒ­ã‚»ã‚¹IDå–å¾—ç”¨

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger(__name__)

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹è¾æ›¸ ã€æ”¹å–„â‘£ã€‘
performance_metrics = {
    'extract': {'time': 0, 'memory': 0},
    'transform': {'time': 0, 'memory': 0},
    'quality_check': {'time': 0, 'memory': 0},
    'load': {'time': 0, 'memory': 0}
}

# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¨­å®šï¼ˆå…¨ã‚¿ã‚¹ã‚¯ã«é©ç”¨ï¼‰
default_args = {
    'retries': 3,                              # å¤±æ•—æ™‚3å›ã¾ã§è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤
    'retry_delay': timedelta(minutes=2),       # æœ€åˆã¯2åˆ†å¾…ã¤
    'retry_exponential_backoff': True,         # 2åˆ†â†’4åˆ†â†’8åˆ†ã¨é–“éš”ã‚’åºƒã’ã‚‹
    'max_retry_delay': timedelta(hours=1),     # æœ€å¤§1æ™‚é–“ã¾ã§å¾…ã¤
    'execution_timeout': timedelta(minutes=30), # 30åˆ†ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
}

# ========================================
# Extract: PostgreSQLã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
# ========================================
def extract_from_postgres():
    """
    PostgreSQLã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
    ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: DBæ¥ç¶šã‚¨ãƒ©ãƒ¼ã€ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ã«å¯¾å¿œ
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„: ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã€æ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªè¨ˆæ¸¬
    """
    # ã€æ”¹å–„â‘¡ã€‘å‡¦ç†æ™‚é–“è¨ˆæ¸¬ - é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
    start_time = time.time()
    
    # ã€æ”¹å–„â‘¢ã€‘ãƒ¡ãƒ¢ãƒªè¨ˆæ¸¬ - é–‹å§‹æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
    process = psutil.Process(os.getpid())
    mem_before = process.memory_info().rss / 1024 / 1024  # MBå˜ä½
    
    try:
        logger.info("Starting data extraction from PostgreSQL")
        
        # PostgreSQLã«æ¥ç¶š
        conn = psycopg2.connect(
            host="source-postgres",
            database="sourcedb",
            user="sourceuser",
            password="sourcepass",
            port=5432
        )
        logger.info("Successfully connected to PostgreSQL")
        
        # ã€æ”¹å–„â‘ ã€‘ã‚¯ã‚¨ãƒªæœ€é©åŒ–: SELECT * â†’ å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿å–å¾—
        # å¤‰æ›´å‰: query = "SELECT * FROM sales"
        # å¤‰æ›´å¾Œ: å¿…è¦ãªã‚«ãƒ©ãƒ ï¼ˆid, sale_date, amountï¼‰ã®ã¿å–å¾—
        query = "SELECT id, sale_date, amount FROM sales"
        logger.info("ã€æ”¹å–„â‘ ã€‘æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ: å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿å–å¾—")
        
        df = pd.read_sql(query, conn)
        
        # æ¥ç¶šã‚’é–‰ã˜ã‚‹
        conn.close()
        
        # ãƒ‡ãƒ¼ã‚¿ç¢ºèª
        logger.info(f"Extracted {len(df)} rows")
        print(df.head())
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        df.to_csv('/tmp/extracted_data.csv', index=False)
        logger.info("Data saved to /tmp/extracted_data.csv")
        
    except psycopg2.OperationalError as e:
        logger.error(f"Database connection failed: {e}")
        raise
    
    except psycopg2.DatabaseError as e:
        logger.error(f"Database query failed: {e}")
        raise
    
    except Exception as e:
        logger.error(f"Unexpected error in extract: {e}")
        raise
    
    finally:
        # ã€æ”¹å–„â‘¡ã€‘å‡¦ç†æ™‚é–“è¨ˆæ¸¬ - çµ‚äº†æ™‚åˆ»ã‚’è¨˜éŒ²ã—ã¦çµŒéæ™‚é–“ã‚’è¨ˆç®—
        end_time = time.time()
        elapsed_time = end_time - start_time
        performance_metrics['extract']['time'] = elapsed_time
        logger.info(f"â±ï¸ ã€æ”¹å–„â‘¡ã€‘Extractå‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’")
        
        # ã€æ”¹å–„â‘¢ã€‘ãƒ¡ãƒ¢ãƒªè¨ˆæ¸¬ - çµ‚äº†æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
        mem_after = process.memory_info().rss / 1024 / 1024  # MBå˜ä½
        mem_used = mem_after - mem_before
        performance_metrics['extract']['memory'] = mem_used
        logger.info(f"ğŸ’¾ ã€æ”¹å–„â‘¢ã€‘Extractä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {mem_used:.2f}MB")

# ========================================
# Transform: ãƒ‡ãƒ¼ã‚¿ã‚’åŠ å·¥ã™ã‚‹
# ========================================
def transform_sales_data():
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã™ã‚‹é–¢æ•°ï¼ˆæ—¥ä»˜åˆ¥ã«é›†è¨ˆï¼‰
    ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã€pandaså‡¦ç†ã‚¨ãƒ©ãƒ¼ã«å¯¾å¿œ
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„: æ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªè¨ˆæ¸¬
    """
    # ã€æ”¹å–„â‘¡ã€‘å‡¦ç†æ™‚é–“è¨ˆæ¸¬ - é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
    start_time = time.time()
    
    # ã€æ”¹å–„â‘¢ã€‘ãƒ¡ãƒ¢ãƒªè¨ˆæ¸¬ - é–‹å§‹æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
    process = psutil.Process(os.getpid())
    mem_before = process.memory_info().rss / 1024 / 1024  # MBå˜ä½
    
    try:
        logger.info("Starting data transformation")
        
        # CSVã‚’èª­ã¿è¾¼ã‚€
        df = pd.read_csv('/tmp/extracted_data.csv')
        logger.info(f"Loaded {len(df)} rows for transformation")
        print("Original data:")
        print(df.head())
        
        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼: å¿…è¦ãªã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        required_columns = ['sale_date', 'amount', 'id']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")
        
        # æ—¥ä»˜ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦é›†è¨ˆ
        transformed_df = df.groupby('sale_date').agg({
            'amount': 'sum',
            'id': 'count'
        })

        # ã‚«ãƒ©ãƒ åã‚’å¤‰æ›´
        transformed_df.columns = ['total_amount', 'product_count']
        transformed_df = transformed_df.reset_index()

        # çµæœã‚’ä¿å­˜
        transformed_df.to_csv('/tmp/transformed_data.csv', index=False)
        logger.info("Transformed data saved to /tmp/transformed_data.csv")
        print("Transformed data:")
        print(transformed_df)
        
    except FileNotFoundError as e:
        logger.error(f"Input file not found: {e}")
        raise
    
    except KeyError as e:
        logger.error(f"Required column missing: {e}")
        raise
    
    except ValueError as e:
        logger.error(f"Data validation error: {e}")
        raise
    
    except Exception as e:
        logger.error(f"Unexpected error in transform: {e}")
        raise
    
    finally:
        # ã€æ”¹å–„â‘¡ã€‘å‡¦ç†æ™‚é–“è¨ˆæ¸¬ - çµ‚äº†æ™‚åˆ»ã‚’è¨˜éŒ²ã—ã¦çµŒéæ™‚é–“ã‚’è¨ˆç®—
        end_time = time.time()
        elapsed_time = end_time - start_time
        performance_metrics['transform']['time'] = elapsed_time
        logger.info(f"â±ï¸ ã€æ”¹å–„â‘¡ã€‘Transformå‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’")
        
        # ã€æ”¹å–„â‘¢ã€‘ãƒ¡ãƒ¢ãƒªè¨ˆæ¸¬ - çµ‚äº†æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
        mem_after = process.memory_info().rss / 1024 / 1024  # MBå˜ä½
        mem_used = mem_after - mem_before
        performance_metrics['transform']['memory'] = mem_used
        logger.info(f"ğŸ’¾ ã€æ”¹å–„â‘¢ã€‘Transformä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {mem_used:.2f}MB")

# ========================================
# Data Quality Check: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
# ========================================
def check_data_quality():
    """
    Transformå¾Œã®ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹é–¢æ•°
    
    ãƒã‚§ãƒƒã‚¯é …ç›®:
    1. ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ãƒã‚§ãƒƒã‚¯ - ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ãªã„ã‹
    2. NULLå€¤ãƒã‚§ãƒƒã‚¯ - å¿…é ˆã‚«ãƒ©ãƒ ã«NULLãŒãªã„ã‹
    3. ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯ - ã‚«ãƒ©ãƒ ã®ãƒ‡ãƒ¼ã‚¿å‹ãŒæ­£ã—ã„ã‹
    4. ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯ - ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯çš„ã«ãŠã‹ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‹
    
    å•é¡ŒãŒã‚ã‚Œã°ValueErrorã‚’æŠ•ã’ã¦ã‚¿ã‚¹ã‚¯ã‚’å¤±æ•—ã•ã›ã‚‹
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„: æ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªè¨ˆæ¸¬
    """
    # ã€æ”¹å–„â‘¡ã€‘å‡¦ç†æ™‚é–“è¨ˆæ¸¬ - é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
    start_time = time.time()
    
    # ã€æ”¹å–„â‘¢ã€‘ãƒ¡ãƒ¢ãƒªè¨ˆæ¸¬ - é–‹å§‹æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
    process = psutil.Process(os.getpid())
    mem_before = process.memory_info().rss / 1024 / 1024  # MBå˜ä½
    
    try:
        logger.info("Starting data quality checks")
        
        # Transformå¾Œã®CSVã‚’èª­ã¿è¾¼ã‚€
        df = pd.read_csv('/tmp/transformed_data.csv')
        logger.info(f"Checking {len(df)} rows")
        
        # ========================================
        # ãƒã‚§ãƒƒã‚¯â‘ : ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ãƒã‚§ãƒƒã‚¯
        # ========================================
        logger.info("Check 1/4: Row count check")
        if len(df) == 0:
            raise ValueError("âŒ ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ï¼Extract/Transformå‡¦ç†ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        logger.info(f"âœ“ ä»¶æ•°ãƒã‚§ãƒƒã‚¯OK: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
        
        # ========================================
        # ãƒã‚§ãƒƒã‚¯â‘¡: NULLå€¤ãƒã‚§ãƒƒã‚¯
        # ========================================
        logger.info("Check 2/4: NULL value check")
        
        # sale_dateã®NULLãƒã‚§ãƒƒã‚¯
        if df['sale_date'].isnull().any():
            null_count = df['sale_date'].isnull().sum()
            raise ValueError(f"âŒ sale_dateã«{null_count}ä»¶ã®NULLãŒã‚ã‚Šã¾ã™ï¼")
        
        # total_amountã®NULLãƒã‚§ãƒƒã‚¯
        if df['total_amount'].isnull().any():
            null_count = df['total_amount'].isnull().sum()
            raise ValueError(f"âŒ total_amountã«{null_count}ä»¶ã®NULLãŒã‚ã‚Šã¾ã™ï¼")
        
        # product_countã®NULLãƒã‚§ãƒƒã‚¯
        if df['product_count'].isnull().any():
            null_count = df['product_count'].isnull().sum()
            raise ValueError(f"âŒ product_countã«{null_count}ä»¶ã®NULLãŒã‚ã‚Šã¾ã™ï¼")
        
        logger.info("âœ“ NULLå€¤ãƒã‚§ãƒƒã‚¯OK: ã™ã¹ã¦ã®å¿…é ˆã‚«ãƒ©ãƒ ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨")
        
        # ========================================
        # ãƒã‚§ãƒƒã‚¯â‘¢: ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
        # ========================================
        logger.info("Check 3/4: Data type check")
        
        # total_amountã¯æ•°å€¤å‹ã§ã‚ã‚‹ã¹ã
        if not pd.api.types.is_numeric_dtype(df['total_amount']):
            actual_type = df['total_amount'].dtype
            raise ValueError(f"âŒ total_amountãŒæ•°å€¤å‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆå®Ÿéš›ã®å‹: {actual_type}ï¼‰")
        
        # product_countã‚‚æ•°å€¤å‹ã§ã‚ã‚‹ã¹ã
        if not pd.api.types.is_numeric_dtype(df['product_count']):
            actual_type = df['product_count'].dtype
            raise ValueError(f"âŒ product_countãŒæ•°å€¤å‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆå®Ÿéš›ã®å‹: {actual_type}ï¼‰")
        
        logger.info("âœ“ ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯OK: ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ãŒæ­£ã—ã„å‹")
        
        # ========================================
        # ãƒã‚§ãƒƒã‚¯â‘£: ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
        # ========================================
        logger.info("Check 4/4: Anomaly check")
        
        # å£²ä¸ŠãŒãƒã‚¤ãƒŠã‚¹ã¯ç•°å¸¸
        negative_amounts = df[df['total_amount'] < 0]
        if len(negative_amounts) > 0:
            logger.error(f"ç•°å¸¸ãƒ‡ãƒ¼ã‚¿:\n{negative_amounts}")
            raise ValueError(f"âŒ å£²ä¸Šé‡‘é¡ãŒãƒã‚¤ãƒŠã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãŒ{len(negative_amounts)}ä»¶ã‚ã‚Šã¾ã™ï¼")
        
        # å•†å“æ•°ãŒ0ä»¥ä¸‹ã¯ç•°å¸¸
        invalid_counts = df[df['product_count'] <= 0]
        if len(invalid_counts) > 0:
            logger.error(f"ç•°å¸¸ãƒ‡ãƒ¼ã‚¿:\n{invalid_counts}")
            raise ValueError(f"âŒ å•†å“æ•°ãŒ0ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ãŒ{len(invalid_counts)}ä»¶ã‚ã‚Šã¾ã™ï¼")
        
        # 1æ—¥ã®å£²ä¸ŠãŒ1000ä¸‡å††ã‚’è¶…ãˆãŸã‚‰è­¦å‘Šï¼ˆç•°å¸¸ã«é«˜é¡ï¼‰
        high_amount_data = df[df['total_amount'] > 10000000]
        if len(high_amount_data) > 0:
            logger.warning(f"âš ï¸ ç•°å¸¸ã«é«˜é¡ãªå£²ä¸ŠãŒã‚ã‚Šã¾ã™ï¼ç¢ºèªã—ã¦ãã ã•ã„:")
            logger.warning(f"\n{high_amount_data}")
        
        logger.info("âœ“ ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯OK: ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯çš„ã«å•é¡Œãªã—")
        
        # ========================================
        # ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯å®Œäº†
        # ========================================
        logger.info("=" * 50)
        logger.info("ğŸ‰ All data quality checks passed!")
        logger.info("=" * 50)
        
    except FileNotFoundError as e:
        logger.error(f"Transformå¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
        raise
    
    except ValueError as e:
        # ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¨ãƒ©ãƒ¼ - ã‚¿ã‚¹ã‚¯ã‚’å¤±æ•—ã•ã›ã‚‹
        logger.error(f"ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯å¤±æ•—: {e}")
        raise
    
    except Exception as e:
        logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        raise
    
    finally:
        # ã€æ”¹å–„â‘¡ã€‘å‡¦ç†æ™‚é–“è¨ˆæ¸¬ - çµ‚äº†æ™‚åˆ»ã‚’è¨˜éŒ²ã—ã¦çµŒéæ™‚é–“ã‚’è¨ˆç®—
        end_time = time.time()
        elapsed_time = end_time - start_time
        performance_metrics['quality_check']['time'] = elapsed_time
        logger.info(f"â±ï¸ ã€æ”¹å–„â‘¡ã€‘Quality Checkå‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’")
        
        # ã€æ”¹å–„â‘¢ã€‘ãƒ¡ãƒ¢ãƒªè¨ˆæ¸¬ - çµ‚äº†æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
        mem_after = process.memory_info().rss / 1024 / 1024  # MBå˜ä½
        mem_used = mem_after - mem_before
        performance_metrics['quality_check']['memory'] = mem_used
        logger.info(f"ğŸ’¾ ã€æ”¹å–„â‘¢ã€‘Quality Checkä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {mem_used:.2f}MB")

# ========================================
# Load: ãƒ‡ãƒ¼ã‚¿ã‚’PostgreSQLã«ä¿å­˜
# ========================================
def load_to_postgres():
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’PostgreSQLã«ä¿å­˜ã™ã‚‹é–¢æ•°
    ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã€DBæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼ã«å¯¾å¿œ
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„: æ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªè¨ˆæ¸¬ã€æ”¹å–„åŠ¹æœã‚µãƒãƒªãƒ¼å‡ºåŠ›
    """
    # ã€æ”¹å–„â‘¡ã€‘å‡¦ç†æ™‚é–“è¨ˆæ¸¬ - é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
    start_time = time.time()
    
    # ã€æ”¹å–„â‘¢ã€‘ãƒ¡ãƒ¢ãƒªè¨ˆæ¸¬ - é–‹å§‹æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
    process = psutil.Process(os.getpid())
    mem_before = process.memory_info().rss / 1024 / 1024  # MBå˜ä½
    
    try:
        logger.info("Starting data load to PostgreSQL")
        
        # CSVã‚’èª­ã¿è¾¼ã‚€
        df = pd.read_csv('/tmp/transformed_data.csv')
        logger.info(f"Loading {len(df)} rows to PostgreSQL")
        print("Data to load:")
        print(df)
        
        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        if df.empty:
            raise ValueError("No data to load - DataFrame is empty")
        
        # PostgreSQLã¸ã®æ¥ç¶šã‚’ä½œæˆ
        engine = create_engine('postgresql://sourceuser:sourcepass@source-postgres:5432/sourcedb')
        logger.info("Database engine created")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’PostgreSQLã«ä¿å­˜
        df.to_sql('sales_summary', engine, if_exists='replace', index=False)
        logger.info(f"Successfully loaded {len(df)} rows to sales_summary table")
        
        # æ¥ç¶šã‚’é–‰ã˜ã‚‹
        engine.dispose()
        logger.info("Database connection closed")
        
    except FileNotFoundError as e:
        logger.error(f"Input file not found: {e}")
        raise
    
    except ValueError as e:
        logger.error(f"Data validation error: {e}")
        raise
    
    except Exception as e:
        logger.error(f"Failed to load data to PostgreSQL: {e}")
        raise
    
    finally:
        # ã€æ”¹å–„â‘¡ã€‘å‡¦ç†æ™‚é–“è¨ˆæ¸¬ - çµ‚äº†æ™‚åˆ»ã‚’è¨˜éŒ²ã—ã¦çµŒéæ™‚é–“ã‚’è¨ˆç®—
        end_time = time.time()
        elapsed_time = end_time - start_time
        performance_metrics['load']['time'] = elapsed_time
        logger.info(f"â±ï¸ ã€æ”¹å–„â‘¡ã€‘Loadå‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’")
        
        # ã€æ”¹å–„â‘¢ã€‘ãƒ¡ãƒ¢ãƒªè¨ˆæ¸¬ - çµ‚äº†æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
        mem_after = process.memory_info().rss / 1024 / 1024  # MBå˜ä½
        mem_used = mem_after - mem_before
        performance_metrics['load']['memory'] = mem_used
        logger.info(f"ğŸ’¾ ã€æ”¹å–„â‘¢ã€‘Loadä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {mem_used:.2f}MB")
        
        # ========================================
        # ã€æ”¹å–„â‘£ã€‘ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›
        # ========================================
        logger.info("=" * 70)
        logger.info("ğŸ“Š ã€æ”¹å–„â‘£ã€‘ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã‚µãƒãƒªãƒ¼")
        logger.info("=" * 70)
        
        # åˆè¨ˆæ™‚é–“ã®è¨ˆç®—
        total_time = (
            performance_metrics['extract']['time'] +
            performance_metrics['transform']['time'] +
            performance_metrics['quality_check']['time'] +
            performance_metrics['load']['time']
        )
        
        # åˆè¨ˆãƒ¡ãƒ¢ãƒªã®è¨ˆç®—
        total_memory = (
            performance_metrics['extract']['memory'] +
            performance_metrics['transform']['memory'] +
            performance_metrics['quality_check']['memory'] +
            performance_metrics['load']['memory']
        )
        
        logger.info("â±ï¸ å‡¦ç†æ™‚é–“:")
        logger.info(f"  - Extract:        {performance_metrics['extract']['time']:>8.2f}ç§’")
        logger.info(f"  - Transform:      {performance_metrics['transform']['time']:>8.2f}ç§’")
        logger.info(f"  - Quality Check:  {performance_metrics['quality_check']['time']:>8.2f}ç§’")
        logger.info(f"  - Load:           {performance_metrics['load']['time']:>8.2f}ç§’")
        logger.info(f"  - åˆè¨ˆ:           {total_time:>8.2f}ç§’")
        logger.info("")
        logger.info("ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:")
        logger.info(f"  - Extract:        {performance_metrics['extract']['memory']:>8.2f}MB")
        logger.info(f"  - Transform:      {performance_metrics['transform']['memory']:>8.2f}MB")
        logger.info(f"  - Quality Check:  {performance_metrics['quality_check']['memory']:>8.2f}MB")
        logger.info(f"  - Load:           {performance_metrics['load']['memory']:>8.2f}MB")
        logger.info(f"  - åˆè¨ˆ:           {total_memory:>8.2f}MB")
        logger.info("")
        logger.info("ğŸ¯ ä¸»ãªæ”¹å–„ç‚¹:")
        logger.info("  âœ… ã€æ”¹å–„â‘ ã€‘ã‚¯ã‚¨ãƒªæœ€é©åŒ–: SELECT * â†’ SELECT id, sale_date, amount")
        logger.info("  âœ… ã€æ”¹å–„â‘¡ã€‘å‡¦ç†æ™‚é–“è¨ˆæ¸¬: å„ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œæ™‚é–“ã‚’è¨˜éŒ²")
        logger.info("  âœ… ã€æ”¹å–„â‘¢ã€‘ãƒ¡ãƒ¢ãƒªè¨ˆæ¸¬: å„ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²")
        logger.info("=" * 70)

# ========================================
# DAGå®šç¾©
# ========================================
with DAG(
    dag_id='basic_etl_pipeline_performance',
    default_args=default_args,
    start_date=datetime(2026, 1, 26),
    schedule='0 16 * * *',
    catchup=False,
    tags=['etl', 'postgres', 'performance']
) as dag:
    
    # ã‚¿ã‚¹ã‚¯1: Extractï¼ˆãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼‰
    extract_task = PythonOperator(
        task_id='extract_from_postgres',
        python_callable=extract_from_postgres
    )
    
    # ã‚¿ã‚¹ã‚¯2: Transformï¼ˆãƒ‡ãƒ¼ã‚¿å¤‰æ›ï¼‰
    transform_task = PythonOperator(
        task_id='transform_sales_data',
        python_callable=transform_sales_data
    )
    
    # ã‚¿ã‚¹ã‚¯3: Data Quality Checkï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ï¼‰
    quality_check_task = PythonOperator(
        task_id='check_data_quality',
        python_callable=check_data_quality
    )
    
    # ã‚¿ã‚¹ã‚¯4: Loadï¼ˆãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼‰
    load_task = PythonOperator(
        task_id='load_to_postgres',
        python_callable=load_to_postgres
    )
    
    # ã‚¿ã‚¹ã‚¯ã®ä¾å­˜é–¢ä¿‚ã‚’å®šç¾©ï¼ˆå®Ÿè¡Œé †åºï¼‰
    # Extract â†’ Transform â†’ Quality Check â†’ Load
    extract_task >> transform_task >> quality_check_task >> load_task
