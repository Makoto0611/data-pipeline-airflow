# ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£… - ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ç‰ˆ
# ç›®çš„: PostgreSQLã®salesãƒ‡ãƒ¼ã‚¿ã‚’æ—¥ä»˜åˆ¥ã«é›†è¨ˆã—ã¦BigQueryã¸é€ã‚‹
# Phase 3: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ã‚’è¿½åŠ 
# å­¦ç¿’æ—¥: 2026-01-23

"""
========================================
ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯è¿½åŠ ãƒã‚¤ãƒ³ãƒˆä¸€è¦§
========================================

ã€è¿½åŠ â‘ ã€‘check_data_qualityé–¢æ•°ã®è¿½åŠ 
    å ´æ‰€: 106-203è¡Œç›®ä»˜è¿‘
    å†…å®¹: Transformå¾Œã®ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹é–¢æ•°
    åŠ¹æœ: ä¸é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ãŒLoadã•ã‚Œã‚‹å‰ã«ã‚¨ãƒ©ãƒ¼ã‚’æ¤œå‡º

ã€è¿½åŠ â‘¡ã€‘4ç¨®é¡ã®å“è³ªãƒã‚§ãƒƒã‚¯
    ãƒã‚§ãƒƒã‚¯â‘ : ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ãªã„ã‹ï¼‰
    ãƒã‚§ãƒƒã‚¯â‘¡: NULLå€¤ãƒã‚§ãƒƒã‚¯ï¼ˆå¿…é ˆã‚«ãƒ©ãƒ ã«NULLãŒãªã„ã‹ï¼‰
    ãƒã‚§ãƒƒã‚¯â‘¢: ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯ï¼ˆã‚«ãƒ©ãƒ ã®ãƒ‡ãƒ¼ã‚¿å‹ãŒæ­£ã—ã„ã‹ï¼‰
    ãƒã‚§ãƒƒã‚¯â‘£: ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯çš„ã«ãŠã‹ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‹ï¼‰

ã€è¿½åŠ â‘¢ã€‘å“è³ªãƒã‚§ãƒƒã‚¯ã‚¿ã‚¹ã‚¯ã®DAGã¸ã®çµ„ã¿è¾¼ã¿
    å ´æ‰€: DAGå®šç¾©éƒ¨åˆ†
    å†…å®¹: Transformã¨Loadã®é–“ã«quality_check_taskã‚’æŒ¿å…¥
    åŠ¹æœ: Extract â†’ Transform â†’ Quality Check â†’ Load ã®é †ã§å®Ÿè¡Œ

ã€è¿½åŠ â‘£ã€‘ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åœæ­¢
    å†…å®¹: å“è³ªãƒã‚§ãƒƒã‚¯å¤±æ•—æ™‚ã«ValueErrorã‚’ç™ºç”Ÿã•ã›ã‚‹
    åŠ¹æœ: ä¸é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ãŒDBã«ä¿å­˜ã•ã‚Œã‚‹ã“ã¨ã‚’é˜²ã
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
import psycopg2
import pandas as pd
from sqlalchemy import create_engine
import logging

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger(__name__)

# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¨­å®šï¼ˆå…¨ã‚¿ã‚¹ã‚¯ã«é©ç”¨ï¼‰
default_args = {
    'retries': 3,                              # å¤±æ•—æ™‚3å›ã¾ã§è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤
    'retry_delay': timedelta(minutes=2),       # æœ€åˆã¯2åˆ†å¾…ã¤
    'retry_exponential_backoff': True,         # 2åˆ†â†’4åˆ†â†’8åˆ†ã¨é–“éš”ã‚’åºƒã’ã‚‹
    'max_retry_delay': timedelta(hours=1),     # æœ€å¤§1æ™‚é–“ã¾ã§å¾…ã¤
    'execution_timeout': timedelta(minutes=30), # 30åˆ†ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
}

# ========================================
# Extract: PostgreSQLã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
# ========================================
def extract_from_postgres():
    """
    PostgreSQLã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
    ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: DBæ¥ç¶šã‚¨ãƒ©ãƒ¼ã€ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ã«å¯¾å¿œ
    """
    try:
        logger.info("Starting data extraction from PostgreSQL")
        
        # PostgreSQLã«æ¥ç¶š
        conn = psycopg2.connect(
            host="source-postgres",
            database="sourcedb",
            user="sourceuser",
            password="sourcepass",
            port=5432
        )
        logger.info("Successfully connected to PostgreSQL")
        
        # salesãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        df = pd.read_sql(query, conn)
        
        # æ¥ç¶šã‚’é–‰ã˜ã‚‹
        conn.close()
        
        # ãƒ‡ãƒ¼ã‚¿ç¢ºèª
        logger.info(f"Extracted {len(df)} rows")
        print(df.head())
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        df.to_csv('/tmp/extracted_data.csv', index=False)
        logger.info("Data saved to /tmp/extracted_data.csv")
        
    except psycopg2.OperationalError as e:
        logger.error(f"Database connection failed: {e}")
        raise
    
    except psycopg2.DatabaseError as e:
        logger.error(f"Database query failed: {e}")
        raise
    
    except Exception as e:
        logger.error(f"Unexpected error in extract: {e}")
        raise

# ========================================
# Transform: ãƒ‡ãƒ¼ã‚¿ã‚’åŠ å·¥ã™ã‚‹
# ========================================
def transform_sales_data():
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã™ã‚‹é–¢æ•°ï¼ˆæ—¥ä»˜åˆ¥ã«é›†è¨ˆï¼‰
    ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã€pandaså‡¦ç†ã‚¨ãƒ©ãƒ¼ã«å¯¾å¿œ
    """
    try:
        logger.info("Starting data transformation")
        
        # CSVã‚’èª­ã¿è¾¼ã‚€
        df = pd.read_csv('/tmp/extracted_data.csv')
        logger.info(f"Loaded {len(df)} rows for transformation")
        print("Original data:")
        print(df.head())
        
        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼: å¿…è¦ãªã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        required_columns = ['sale_date', 'amount', 'id']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")
        
        # æ—¥ä»˜ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦é›†è¨ˆ
        transformed_df = df.groupby('sale_date').agg({
            'amount': 'sum',
            'id': 'count'
        })

        # ã‚«ãƒ©ãƒ åã‚’å¤‰æ›´
        transformed_df.columns = ['total_amount', 'product_count']
        transformed_df = transformed_df.reset_index()

        # çµæœã‚’ä¿å­˜
        transformed_df.to_csv('/tmp/transformed_data.csv', index=False)
        logger.info("Transformed data saved to /tmp/transformed_data.csv")
        print("Transformed data:")
        print(transformed_df)
        
    except FileNotFoundError as e:
        logger.error(f"Input file not found: {e}")
        raise
    
    except KeyError as e:
        logger.error(f"Required column missing: {e}")
        raise
    
    except ValueError as e:
        logger.error(f"Data validation error: {e}")
        raise
    
    except Exception as e:
        logger.error(f"Unexpected error in transform: {e}")
        raise

# ========================================
# Data Quality Check: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ ã€è¿½åŠ â‘ ã€‘
# ========================================
def check_data_quality():  # ã€è¿½åŠ â‘ ã€‘æ–°è¦é–¢æ•°
    """
    Transformå¾Œã®ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹é–¢æ•°
    
    ãƒã‚§ãƒƒã‚¯é …ç›®:
    1. ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ãƒã‚§ãƒƒã‚¯ - ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ãªã„ã‹
    2. NULLå€¤ãƒã‚§ãƒƒã‚¯ - å¿…é ˆã‚«ãƒ©ãƒ ã«NULLãŒãªã„ã‹
    3. ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯ - ã‚«ãƒ©ãƒ ã®ãƒ‡ãƒ¼ã‚¿å‹ãŒæ­£ã—ã„ã‹
    4. ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯ - ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯çš„ã«ãŠã‹ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‹
    
    å•é¡ŒãŒã‚ã‚Œã°ValueErrorã‚’æŠ•ã’ã¦ã‚¿ã‚¹ã‚¯ã‚’å¤±æ•—ã•ã›ã‚‹
    """
    try:
        logger.info("Starting data quality checks")
        
        # Transformå¾Œã®CSVã‚’èª­ã¿è¾¼ã‚€
        df = pd.read_csv('/tmp/transformed_data.csv')
        logger.info(f"Checking {len(df)} rows")
        
        # ========================================
        # ã€è¿½åŠ â‘¡ã€‘ãƒã‚§ãƒƒã‚¯â‘ : ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ãƒã‚§ãƒƒã‚¯
        # ========================================
        logger.info("Check 1/4: Row count check")
        if len(df) == 0:
            raise ValueError("âŒ ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ï¼Extract/Transformå‡¦ç†ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        logger.info(f"âœ“ ä»¶æ•°ãƒã‚§ãƒƒã‚¯OK: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
        
        # ========================================
        # ã€è¿½åŠ â‘¡ã€‘ãƒã‚§ãƒƒã‚¯â‘¡: NULLå€¤ãƒã‚§ãƒƒã‚¯
        # ========================================
        logger.info("Check 2/4: NULL value check")
        
        # sale_dateã®NULLãƒã‚§ãƒƒã‚¯
        if df['sale_date'].isnull().any():
            null_count = df['sale_date'].isnull().sum()
            raise ValueError(f"âŒ sale_dateã«{null_count}ä»¶ã®NULLãŒã‚ã‚Šã¾ã™ï¼")
        
        # total_amountã®NULLãƒã‚§ãƒƒã‚¯
        if df['total_amount'].isnull().any():
            null_count = df['total_amount'].isnull().sum()
            raise ValueError(f"âŒ total_amountã«{null_count}ä»¶ã®NULLãŒã‚ã‚Šã¾ã™ï¼")
        
        # product_countã®NULLãƒã‚§ãƒƒã‚¯
        if df['product_count'].isnull().any():
            null_count = df['product_count'].isnull().sum()
            raise ValueError(f"âŒ product_countã«{null_count}ä»¶ã®NULLãŒã‚ã‚Šã¾ã™ï¼")
        
        logger.info("âœ“ NULLå€¤ãƒã‚§ãƒƒã‚¯OK: ã™ã¹ã¦ã®å¿…é ˆã‚«ãƒ©ãƒ ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨")
        
        # ========================================
        # ã€è¿½åŠ â‘¡ã€‘ãƒã‚§ãƒƒã‚¯â‘¢: ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
        # ========================================
        logger.info("Check 3/4: Data type check")
        
        # total_amountã¯æ•°å€¤å‹ã§ã‚ã‚‹ã¹ã
        if not pd.api.types.is_numeric_dtype(df['total_amount']):
            actual_type = df['total_amount'].dtype
            raise ValueError(f"âŒ total_amountãŒæ•°å€¤å‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆå®Ÿéš›ã®å‹: {actual_type}ï¼‰")
        
        # product_countã‚‚æ•°å€¤å‹ã§ã‚ã‚‹ã¹ã
        if not pd.api.types.is_numeric_dtype(df['product_count']):
            actual_type = df['product_count'].dtype
            raise ValueError(f"âŒ product_countãŒæ•°å€¤å‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆå®Ÿéš›ã®å‹: {actual_type}ï¼‰")
        
        logger.info("âœ“ ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯OK: ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ãŒæ­£ã—ã„å‹")
        
        # ========================================
        # ã€è¿½åŠ â‘¡ã€‘ãƒã‚§ãƒƒã‚¯â‘£: ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
        # ========================================
        logger.info("Check 4/4: Anomaly check")
        
        # å£²ä¸ŠãŒãƒã‚¤ãƒŠã‚¹ã¯ç•°å¸¸
        negative_amounts = df[df['total_amount'] < 0]
        if len(negative_amounts) > 0:
            logger.error(f"ç•°å¸¸ãƒ‡ãƒ¼ã‚¿:\n{negative_amounts}")
            raise ValueError(f"âŒ å£²ä¸Šé‡‘é¡ãŒãƒã‚¤ãƒŠã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãŒ{len(negative_amounts)}ä»¶ã‚ã‚Šã¾ã™ï¼")
        
        # å•†å“æ•°ãŒ0ä»¥ä¸‹ã¯ç•°å¸¸
        invalid_counts = df[df['product_count'] <= 0]
        if len(invalid_counts) > 0:
            logger.error(f"ç•°å¸¸ãƒ‡ãƒ¼ã‚¿:\n{invalid_counts}")
            raise ValueError(f"âŒ å•†å“æ•°ãŒ0ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ãŒ{len(invalid_counts)}ä»¶ã‚ã‚Šã¾ã™ï¼")
        
        # 1æ—¥ã®å£²ä¸ŠãŒ1000ä¸‡å††ã‚’è¶…ãˆãŸã‚‰è­¦å‘Šï¼ˆç•°å¸¸ã«é«˜é¡ï¼‰
        high_amount_data = df[df['total_amount'] > 10000000]
        if len(high_amount_data) > 0:
            logger.warning(f"âš ï¸ ç•°å¸¸ã«é«˜é¡ãªå£²ä¸ŠãŒã‚ã‚Šã¾ã™ï¼ç¢ºèªã—ã¦ãã ã•ã„:")
            logger.warning(f"\n{high_amount_data}")
        
        logger.info("âœ“ ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯OK: ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯çš„ã«å•é¡Œãªã—")
        
        # ========================================
        # ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯å®Œäº†
        # ========================================
        logger.info("=" * 50)
        logger.info("ğŸ‰ All data quality checks passed!")
        logger.info("=" * 50)
        
    except FileNotFoundError as e:
        logger.error(f"Transformå¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
        raise
    
    except ValueError as e:
        # ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¨ãƒ©ãƒ¼ - ã‚¿ã‚¹ã‚¯ã‚’å¤±æ•—ã•ã›ã‚‹
        logger.error(f"ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯å¤±æ•—: {e}")
        raise
    
    except Exception as e:
        logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        raise

# ========================================
# Load: ãƒ‡ãƒ¼ã‚¿ã‚’PostgreSQLã«ä¿å­˜
# ========================================
def load_to_postgres():
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’PostgreSQLã«ä¿å­˜ã™ã‚‹é–¢æ•°
    ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã€DBæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼ã«å¯¾å¿œ
    """
    try:
        logger.info("Starting data load to PostgreSQL")
        
        # CSVã‚’èª­ã¿è¾¼ã‚€
        df = pd.read_csv('/tmp/transformed_data.csv')
        logger.info(f"Loading {len(df)} rows to PostgreSQL")
        print("Data to load:")
        print(df)
        
        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        if df.empty:
            raise ValueError("No data to load - DataFrame is empty")
        
        # PostgreSQLã¸ã®æ¥ç¶šã‚’ä½œæˆ
        engine = create_engine('postgresql://sourceuser:sourcepass@source-postgres:5432/sourcedb')
        logger.info("Database engine created")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’PostgreSQLã«ä¿å­˜
        df.to_sql('sales_summary', engine, if_exists='replace', index=False)
        logger.info(f"Successfully loaded {len(df)} rows to sales_summary table")
        
        # æ¥ç¶šã‚’é–‰ã˜ã‚‹
        engine.dispose()
        logger.info("Database connection closed")
        
    except FileNotFoundError as e:
        logger.error(f"Input file not found: {e}")
        raise
    
    except ValueError as e:
        logger.error(f"Data validation error: {e}")
        raise
    
    except Exception as e:
        logger.error(f"Failed to load data to PostgreSQL: {e}")
        raise

# ========================================
# DAGå®šç¾©
# ========================================
with DAG(
    dag_id='basic_etl_pipeline_data_quality',
    default_args=default_args,
    start_date=datetime(2026, 1, 23),
    schedule='0 16 * * *',
    catchup=False,
    tags=['etl', 'postgres', 'data-quality']
) as dag:
    
    # ã‚¿ã‚¹ã‚¯1: Extractï¼ˆãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼‰
    extract_task = PythonOperator(
        task_id='extract_from_postgres',
        python_callable=extract_from_postgres
    )
    
    # ã‚¿ã‚¹ã‚¯2: Transformï¼ˆãƒ‡ãƒ¼ã‚¿å¤‰æ›ï¼‰
    transform_task = PythonOperator(
        task_id='transform_sales_data',
        python_callable=transform_sales_data
    )
    
    # ã€è¿½åŠ â‘¢ã€‘ã‚¿ã‚¹ã‚¯3: Data Quality Checkï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ï¼‰
    quality_check_task = PythonOperator(  # ã€è¿½åŠ â‘¢ã€‘æ–°è¦ã‚¿ã‚¹ã‚¯
        task_id='check_data_quality',
        python_callable=check_data_quality
    )
    
    # ã‚¿ã‚¹ã‚¯4: Loadï¼ˆãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼‰
    load_task = PythonOperator(
        task_id='load_to_postgres',
        python_callable=load_to_postgres
    )
    
    # ã€è¿½åŠ â‘¢ã€‘ã‚¿ã‚¹ã‚¯ã®ä¾å­˜é–¢ä¿‚ã‚’å®šç¾©ï¼ˆå®Ÿè¡Œé †åºï¼‰
    # Extract â†’ Transform â†’ Quality Check â†’ Load
    extract_task >> transform_task >> quality_check_task >> load_task  # ã€è¿½åŠ â‘¢ã€‘Quality Checkã‚’æŒ¿å…¥
